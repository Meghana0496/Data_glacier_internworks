import os
import time

#Size of the file
os.path.getsize('C:/Users/Data Glacier Intern/Data/Rate.csv')
1968829442

Read in the data with Dask
from dask import dataframe as dd
start = time.time()
dask_df = dd.read_csv('C:/Users/Data Glacier Intern/Data/Rate.csv')
end = time.time()
print("Read csv with dask: ",(end-start),"sec")
Read csv with dask:  0.011967897415161133 sec

Read in the data with Pandas
import pandas as pd
start = time.time()
df = pd.read_csv('C:/Users/Data Glacier Intern/Data/Rate.csv')
end = time.time()
print("Read csv with pandas: ",(end-start),"sec")
Read csv with pandas:  28.48867106437683 sec

Read in the data with Modin and Ray
import modin.pandas as pd
import ray
ray.shutdown()
ray.init()
start = time.time()


df = pd.read_csv('C:/Users/Data Glacier Intern/Data/Rate.csv')
end = time.time()
print("Read csv with modin and ray: ",(end-start),"sec")
2021-04-16 13:16:20,499	INFO services.py:1171 -- View the Ray dashboard at http://127.0.0.1:8265
Read csv with modin and ray:  24.314270734786987 sec
Here Dask is better than Pandas, Modin and Ray, with the least reading time of 0.012 sec

from dask import dataframe as dd

df = dd.read_csv('C:/Users/Data Glacier Intern/Data/Rate.csv',delimiter=',')
df.info()

<class 'dask.dataframe.core.DataFrame'>

Columns: 24 entries, BusinessYear to RowNumber
dtypes: object(10), float64(9), int64(5)

#No. of Rows

len(df.index)
12694445

#No, of Columns
len(df.columns)
24

# remove special character
df.columns=df.columns.str.replace('[#,@,&]','')
FutureWarning: The default value of regex will change from True to False in a future version.
#To remove white space from columns
df.columns = df.columns.str.replace(' ', '')
data=df.columns
data
Index(['BusinessYear', 'StateCode', 'IssuerId', 'SourceName', 'VersionNum',
       'ImportDate', 'IssuerId2', 'FederalTIN', 'RateEffectiveDate',
       'RateExpirationDate', 'PlanId', 'RatingAreaId', 'Tobacco', 'Age',
       'IndividualRate', 'IndividualTobaccoRate', 'Couple',
       'PrimarySubscriberAndOneDependent', 'PrimarySubscriberAndTwoDependents',
       'PrimarySubscriberAndThreeOrMoreDependents', 'CoupleAndOneDependent',
       'CoupleAndTwoDependents', 'CoupleAndThreeOrMoreDependents',
       'RowNumber'],
      dtype='object')
Validation
import logging
import os
import subprocess
import yaml
import pandas as pd
import datetime 
import gc
import re
%%writefile utility.py

def read_config_file(filepath):
    with open(filepath, 'r') as stream:
        try:
            return yaml.load(stream, Loader=yaml.Loader)
        except yaml.YAMLError as exc:
            logging.error(exc)

def col_header_val(df,table_config):
    df.columns = df.columns.str.lower()
    df.columns = df.columns.str.replace('[^\w]','_',regex=True)
    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))
    df.columns = list(map(lambda x: replacer(x,'_'), list(df.columns)))
    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))
    expected_col.sort()
    df.columns =list(map(lambda x: x.lower(), list(df.columns)))
    df = df.reindex(sorted(df.columns), axis=1)
    if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):
        print("column name and column length validation passed")
        return 1
    else:
        print("column name and column length validation failed")
        mismatched_columns_file = list(set(df.columns).difference(expected_col))
        print("Following File columns are not in the YAML file",mismatched_columns_file)
        missing_YAML_file = list(set(expected_col).difference(df.columns))
        print("Following YAML columns are not in the file uploaded",missing_YAML_file)
        logging.info(f'df columns: {df.columns}')
        logging.info(f'expected columns: {expected_col}')
        return 0
Overwriting utility.py
%%writefile store.yaml
file_type: csv
dataset_name: file
file_name: Rate
table_name: edsurv
inbound_delimiter: ","
outbound_delimiter: "|"
skip_leading_rows: 1
columns: 
    - BusinessYear
      - StateCode
      - IssuerId
      - SourceName
      - VersionNum
      - ImportDate
      - IssuerId2
      - FederalTIN
      - RateEffectiveDate
      - RateExpirationDate
      - PlanId
      - RatingAreaId
      - Tobacco
      - Age
      - IndividualRate
      - IndividualTobaccoRate
      - Couple
      - PrimarySubscriberAndOneDependent
      - PrimarySubscriberAndTwoDependents
      - PrimarySubscriberAndThreeOrMoreDependents
      - CoupleAndOneDependent
      - CoupleAndTwoDependents
      - CoupleAndThreeOrMoreDependents
      - RowNumber
Overwriting store.yaml


# Reading config file
import utility as util
config_data = util.read_config_file("store.yaml")
#data of config file
config_data
{'file_type': 'csv',
 'dataset_name': 'file',
 'file_name': 'Rate',
 'table_name': 'edsurv',
 'inbound_delimiter': ',',
 'outbound_delimiter': '|',
 'skip_leading_rows': 1,
 'columns': ['BusinessYear - StateCode - IssuerId - SourceName - VersionNum - ImportDate - IssuerId2 - FederalTIN - RateEffectiveDate - RateExpirationDate - PlanId - RatingAreaId - Tobacco - Age - IndividualRate - IndividualTobaccoRate - Couple - PrimarySubscriberAndOneDependent - PrimarySubscriberAndTwoDependents - PrimarySubscriberAndThreeOrMoreDependents - CoupleAndOneDependent - CoupleAndTwoDependents - CoupleAndThreeOrMoreDependents - RowNumber']}


# Reading process of the file using Dask
from dask import dataframe as dd
df_sample = dd.read_csv('C:/Users/Data Glacier Intern/Data/Rate.csv',delimiter=',')
df_sample.head()

BusinessYear	StateCode	IssuerId	SourceName	VersionNum	ImportDate	IssuerId2	FederalTIN	RateEffectiveDate	RateExpirationDate	...	IndividualRate	IndividualTobaccoRate	Couple	PrimarySubscriberAndOneDependent	PrimarySubscriberAndTwoDependents	PrimarySubscriberAndThreeOrMoreDependents	CoupleAndOneDependent	CoupleAndTwoDependents	CoupleAndThreeOrMoreDependents	RowNumber
0	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	29.00	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	14
1	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	36.95	NaN	73.9	107.61	107.61	107.61	144.56	144.56	144.56	14
2	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	36.95	NaN	73.9	107.61	107.61	107.61	144.56	144.56	144.56	15
3	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	32.00	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	15
4	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	32.00	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	16
5 rows × 24 columns

#Reading the file using config file
file_type = config_data['file_type']
source_file = "C:/Users/Desktop/" + config_data['file_name'] + f'.{file_type}'

import pandas as pd
df = pd.read_csv(source_file,config_data['inbound_delimiter'])
df.head()

BusinessYear	StateCode	IssuerId	SourceName	VersionNum	ImportDate	IssuerId2	FederalTIN	RateEffectiveDate	RateExpirationDate	...	IndividualRate	IndividualTobaccoRate	Couple	PrimarySubscriberAndOneDependent	PrimarySubscriberAndTwoDependents	PrimarySubscriberAndThreeOrMoreDependents	CoupleAndOneDependent	CoupleAndTwoDependents	CoupleAndThreeOrMoreDependents	RowNumber
0	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	29.00	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	14
1	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	36.95	NaN	73.9	107.61	107.61	107.61	144.56	144.56	144.56	14
2	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	36.95	NaN	73.9	107.61	107.61	107.61	144.56	144.56	144.56	15
3	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	32.00	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	15
4	2014	AK	21989	HIOS	6	2014-03-19 07:06:49	21989	93-0438772	2014-01-01	2014-12-31	...	32.00	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	16
5 rows × 24 columns

#validating the header of the file
util.col_header_val(df,config_data)
column name and column length validation failed
Following File columns are not in the YAML file ['planid', 'rateexpirationdate', 'rateeffectivedate', 'importdate', 'primarysubscriberandthreeormoredependents', 'rownumber', 'federaltin', 'issuerid', 'individualrate', 'issuerid2', 'age', 'ratingareaid', 'couple', 'individualtobaccorate', 'statecode', 'coupleandtwodependents', 'tobacco', 'sourcename', 'primarysubscriberandonedependent', 'coupleandthreeormoredependents', 'businessyear', 'coupleandonedependent', 'primarysubscriberandtwodependents', 'versionnum']
Following YAML columns are not in the file uploaded ['businessyear - statecode - issuerid - sourcename - versionnum - importdate - issuerid2 - federaltin - rateeffectivedate - rateexpirationdate - planid - ratingareaid - tobacco - age - individualrate - individualtobaccorate - couple - primarysubscriberandonedependent - primarysubscriberandtwodependents - primarysubscriberandthreeormoredependents - coupleandonedependent - coupleandtwodependents - coupleandthreeormoredependents - rownumber']
0
print("columns of files are:" ,df.columns)
print("columns of YAML are:" ,config_data['columns'])
columns of files are: Index(['businessyear', 'statecode', 'issuerid', 'sourcename', 'versionnum',
       'importdate', 'issuerid2', 'federaltin', 'rateeffectivedate',
       'rateexpirationdate', 'planid', 'ratingareaid', 'tobacco', 'age',
       'individualrate', 'individualtobaccorate', 'couple',
       'primarysubscriberandonedependent', 'primarysubscriberandtwodependents',
       'primarysubscriberandthreeormoredependents', 'coupleandonedependent',
       'coupleandtwodependents', 'coupleandthreeormoredependents',
       'rownumber'],
      dtype='object')
columns of YAML are: ['BusinessYear - StateCode - IssuerId - SourceName - VersionNum - ImportDate - IssuerId2 - FederalTIN - RateEffectiveDate - RateExpirationDate - PlanId - RatingAreaId - Tobacco - Age - IndividualRate - IndividualTobaccoRate - Couple - PrimarySubscriberAndOneDependent - PrimarySubscriberAndTwoDependents - PrimarySubscriberAndThreeOrMoreDependents - CoupleAndOneDependent - CoupleAndTwoDependents - CoupleAndThreeOrMoreDependents - RowNumber']
if util.col_header_val(df,config_data)==0:
    print("validation failed")
else:
    print("col validation passed")
column name and column length validation failed
Following File columns are not in the YAML file ['planid', 'rateexpirationdate', 'rateeffectivedate', 'importdate', 'primarysubscriberandthreeormoredependents', 'rownumber', 'federaltin', 'issuerid', 'individualrate', 'issuerid2', 'age', 'ratingareaid', 'couple', 'individualtobaccorate', 'statecode', 'coupleandtwodependents', 'tobacco', 'sourcename', 'primarysubscriberandonedependent', 'coupleandthreeormoredependents', 'businessyear', 'coupleandonedependent', 'primarysubscriberandtwodependents', 'versionnum']
Following YAML columns are not in the file uploaded ['businessyear - statecode - issuerid - sourcename - versionnum - importdate - issuerid2 - federaltin - rateeffectivedate - rateexpirationdate - planid - ratingareaid - tobacco - age - individualrate - individualtobaccorate - couple - primarysubscriberandonedependent - primarysubscriberandtwodependents - primarysubscriberandthreeormoredependents - coupleandonedependent - coupleandtwodependents - coupleandthreeormoredependents - rownumber']
validation failed
import datetime
import csv
import gzip

from dask import dataframe as dd
df = dd.read_csv('C:/User/Data Glacier Intern/Data/Rate.csv',delimiter=',')

# Write csv in gz format in pipe separated text file (|)
df.to_csv('Rate.csv.gz',
          sep='|',
          header=True,
          index=False,
          quoting=csv.QUOTE_ALL,
          compression='gzip',
          quotechar='"',
          doublequote=True,
          line_terminator='\n')

#number of files in gz format folder
import os
entries = os.listdir('C:/Users/Data Glacier Intern/Data/Rate.csv.gz/')
for entry in entries:
    print(entry)
00.part
01.part
02.part
03.part
04.part
05.part
06.part
07.part
08.part
09.part
10.part
11.part
12.part
13.part
14.part
15.part
16.part
17.part
18.part
19.part
20.part
21.part
22.part
23.part
24.part
25.part
26.part
27.part
28.part
29.part
30.part
#size of the gz format folder
os.path.getsize('C:/Users/Data Glacier Intern/Data/Rate.csv.gz')
